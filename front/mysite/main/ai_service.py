import os
import asyncio
import torch
import numpy as np
from datetime import datetime
from typing import List, Dict, Any

from django.conf import settings
from dotenv import load_dotenv

# =======================================================
# [Fix] Import: ì¡´ì¬í•˜ëŠ” íŒ¨í‚¤ì§€ë§Œ í™•ì‹¤í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°
# =======================================================
# 1. LangChain Core (ì„¤ì¹˜ëœ 1.2.7 ë²„ì „ í™œìš©)
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.documents import Document

# 2. Embeddings & GenAI
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_chroma import Chroma

# 3. Community Modules (BM25, CrossEncoder)
# ë§Œì•½ ê²½ë¡œ ë¬¸ì œ ë°œìƒ ì‹œ ì§ì ‘ì ì¸ ëŒ€ì²´ ë¡œì§ì„ ìœ„í•´ try-import
try:
    from langchain_community.retrievers import BM25Retriever
except ImportError:
    BM25Retriever = None # ì—†ì„ ê²½ìš° ì§ì ‘ êµ¬í˜„ ë¡œì§ ì‚¬ìš© ëŒ€ë¹„

try:
    from langchain_community.cross_encoders import HuggingFaceCrossEncoder
except ImportError:
    HuggingFaceCrossEncoder = None

# =======================================================
# [ì„¤ì •] í™˜ê²½ ë³€ìˆ˜ ë° ëª¨ë¸ ë¡œë“œ
# =======================================================
load_dotenv()

PERSIST_DIRECTORY = "./RAG/chromadb_report_L1_to_L4"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

print(f"ğŸ“‚ [System] AI ì„œë¹„ìŠ¤ ì´ˆê¸°í™” (Direct Logic Mode)")
print(f"âš¡ [Device] {DEVICE.upper()}")

# 1. ì„ë² ë”© ëª¨ë¸
embeddings = HuggingFaceEmbeddings(
    model_name="BAAI/bge-m3",
    model_kwargs={'device': DEVICE},
    encode_kwargs={'normalize_embeddings': True}
)

# 2. ë¦¬ë­ì»¤ ëª¨ë¸ (CrossEncoder)
# Import ì—ëŸ¬ê°€ ë‚˜ë”ë¼ë„ sentence_transformersë¥¼ ì§ì ‘ ì‚¬ìš©í•˜ì—¬ êµ¬í˜„ ê°€ëŠ¥
reranker_model = None
try:
    if HuggingFaceCrossEncoder:
        reranker_model = HuggingFaceCrossEncoder(
            model_name="dragonkue/bge-reranker-v2-m3-ko",
            model_kwargs={'device': DEVICE}
        )
    else:
        # Fallback: sentence_transformers ì§ì ‘ ì‚¬ìš©
        from sentence_transformers import CrossEncoder
        reranker_model = CrossEncoder("dragonkue/bge-reranker-v2-m3-ko", device=DEVICE)
except Exception as e:
    print(f"âš ï¸ Reranker ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

# 3. LLM ì„¤ì •
llm = ChatGoogleGenerativeAI(
    model="gemini-3.0-flash", 
    temperature=0.2,
    google_api_key=os.getenv("GEMINI_API_KEY"),
    streaming=True
)

# 4. VectorStore ì—°ê²°
vector_store = Chroma(
    persist_directory=PERSIST_DIRECTORY,
    embedding_function=embeddings
)

# =======================================================
# [Logic] Direct Hybrid Search Implementation
# ë¼ì´ë¸ŒëŸ¬ë¦¬(EnsembleRetriever ë“±) ì—†ì´ ì§ì ‘ ë¡œì§ êµ¬í˜„
# =======================================================

class DirectSearchEngine:
    _docs_cache: Dict[str, List[Document]] = {}
    _is_loaded = False

    @classmethod
    def load_cache(cls):
        if cls._is_loaded: return
        
        # [ë””ë²„ê¹… 1] ê²½ë¡œ í™•ì¸
        abs_path = os.path.abspath(PERSIST_DIRECTORY)
        print(f"ğŸ§ [Debug] DB ê²½ë¡œ í™•ì¸: {abs_path}")
        if not os.path.exists(PERSIST_DIRECTORY):
            print(f"âŒ [Critical] í•´ë‹¹ ê²½ë¡œì— í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤! ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            return

        print("â³ [System] ë¬¸ì„œ ë°ì´í„° ë¡œë”© ë° ìºì‹± ì‹œë„...")
        try:
            # DB ì—°ê²° í…ŒìŠ¤íŠ¸
            data = vector_store.get()
            doc_count = len(data['documents']) if data['documents'] else 0
            
            print(f"ğŸ“Š [Debug] DBì—ì„œ ë°œê²¬ëœ ë¬¸ì„œ ìˆ˜: {doc_count}ê°œ")

            if doc_count == 0:
                print("âš ï¸ [Warning] DBê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ì„ë² ë”©(Ingest)ì´ ì œëŒ€ë¡œ ì•ˆ ëê±°ë‚˜ ê²½ë¡œê°€ í‹€ë ¸ìŠµë‹ˆë‹¤.")
                return
            
            for text, meta in zip(data['documents'], data['metadatas']):
                app = meta.get('app_name', 'Unknown')
                doc = Document(page_content=text, metadata=meta)
                if app not in cls._docs_cache:
                    cls._docs_cache[app] = []
                cls._docs_cache[app].append(doc)
            
            cls._is_loaded = True
            apps_found = list(cls._docs_cache.keys())
            print(f"âœ… [System] ìºì‹± ì™„ë£Œ. ë°œê²¬ëœ ì•± ëª©ë¡: {apps_found}")

        except Exception as e:
            print(f"âŒ [Error] ìºì‹± ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")

    @classmethod
    def get_bm25_retriever(cls, target_apps: List[str]):
        docs = []
        for app in target_apps:
            docs.extend(cls._docs_cache.get(app, []))
        
        if not docs: 
            print(f"âš ï¸ [Debug] '{target_apps}'ì— ëŒ€í•œ ìºì‹œ ë¬¸ì„œê°€ ì—†ìŒ (BM25 ìƒì„± ë¶ˆê°€)")
            return None
        
        if BM25Retriever:
            return BM25Retriever.from_documents(docs)
        return None

    @staticmethod
    def rerank_documents(query: str, docs: List[Document], top_n: int = 5) -> List[Document]:
        if not docs or not reranker_model: return docs[:top_n]
        pairs = [[query, doc.page_content] for doc in docs]
        try:
            if hasattr(reranker_model, 'predict'):
                scores = reranker_model.predict(pairs)
            elif hasattr(reranker_model, 'score'):
                scores = reranker_model.score(pairs)
            else:
                return docs[:top_n]
            
            scored_docs = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)
            return [doc for doc, score in scored_docs[:top_n]]
        except Exception as e:
            print(f"âš ï¸ Reranking Error: {e}")
            return docs[:top_n]

# ì„œë²„ ì‹œì‘ ì‹œ ë¡œë“œ
DirectSearchEngine.load_cache()


# =======================================================
# [ìœ í‹¸] ì§ˆë¬¸ ì˜ë„ ë¶„ì„
# =======================================================
async def analyze_query_intent(query: str) -> Dict:
    query_lower = query.lower()
    intent_filter = {} 
    if any(k in query_lower for k in ["ì¢…í•©", "ì „ì²´", "ì´í‰", "ë¸Œëœë“œ"]):
        intent_filter["an_level"] = {"$in": ["L3", "L4"]}
    elif any(k in query_lower for k in ["ë¶„ê¸°", "íŠ¸ë Œë“œ", "ë™í–¥", "ì¶”ì„¸", "ë¹„êµ"]):
        intent_filter["an_level"] = "L2"
    elif any(k in query_lower for k in ["ë²„ê·¸", "ì˜¤ë¥˜", "ì•ˆë¼", "ì‹¤í–‰", "ì—…ë°ì´íŠ¸", "ë²„ì „"]):
        intent_filter["an_level"] = "L1"
    return intent_filter


# =======================================================
# [Main] ì‘ë‹µ ìƒì„±ê¸°
# =======================================================
async def analyze_query_intent(query: str) -> Dict:
    query_lower = query.lower()
    intent_filter = {} 
    if any(k in query_lower for k in ["ì¢…í•©", "ì „ì²´", "ì´í‰", "ë¸Œëœë“œ"]):
        intent_filter["an_level"] = {"$in": ["L3", "L4"]}
    elif any(k in query_lower for k in ["ë¶„ê¸°", "íŠ¸ë Œë“œ", "ë™í–¥", "ì¶”ì„¸", "ë¹„êµ"]):
        intent_filter["an_level"] = "L2"
    elif any(k in query_lower for k in ["ë²„ê·¸", "ì˜¤ë¥˜", "ì•ˆë¼", "ì‹¤í–‰", "ì—…ë°ì´íŠ¸", "ë²„ì „"]):
        intent_filter["an_level"] = "L1"
    return intent_filter


async def generate_chat_response(query, valid_apps, current_app_name=None):
    if not DirectSearchEngine._is_loaded:
        DirectSearchEngine.load_cache()

    # [ë””ë²„ê¹… 2] ì…ë ¥ê°’ í™•ì¸
    print(f"ğŸ“¥ [Input] ì§ˆë¬¸: {query} | í˜„ì¬ì•±: {current_app_name} | ê¶Œí•œì•±: {valid_apps}")

    target_apps = []
    context_keywords = ["ë‚´ ì•±", "ì´ ì•±", "ì—¬ê¸°", "ìš°ë¦¬ ì•±", "ìš”ì•½", "ë¶„ì„"]
    if current_app_name and (any(k in query for k in context_keywords) or not any(app in query for app in valid_apps if app != current_app_name)):
        target_apps = [current_app_name]
        search_query = f"{current_app_name} {query}"
    else:
        target_apps = valid_apps
        search_query = query
    
    display_query = query 

    if not target_apps:
        yield "ë¶„ì„í•  ì•± ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤."
        return

    level_filter = await analyze_query_intent(search_query)
    
    # í•„í„° ì¡°ê±´ êµ¬ì„±
    app_condition = {}
    if len(target_apps) == 1:
        app_condition = {"app_name": target_apps[0]}
    else:
        app_condition = {"app_name": {"$in": target_apps}}
    
    chroma_filter = {}
    if level_filter:
        chroma_filter = {"$and": [app_condition, level_filter]}
    else:
        chroma_filter = app_condition

    # [ë””ë²„ê¹… 3] ì‹¤ì œ ê²€ìƒ‰ì— ì‚¬ìš©ë˜ëŠ” í•„í„° í™•ì¸
    print(f"ğŸ” [Search] ê²€ìƒ‰ ì¿¼ë¦¬: '{search_query}'")
    print(f"ğŸ” [Search] í•„í„° ì¡°ê±´: {chroma_filter}")

    final_docs = []
    
    try:
        # Step 1: Vector Search
        vector_docs = await asyncio.to_thread(
            vector_store.similarity_search,
            query=search_query,
            k=20,
            filter=chroma_filter
        )
        print(f"  â”” [Result] ë²¡í„° ê²€ìƒ‰ ê²°ê³¼: {len(vector_docs)}ê°œ")
        
        # Step 2: BM25 Search
        bm25_docs = []
        bm25_retriever = DirectSearchEngine.get_bm25_retriever(target_apps)
        if bm25_retriever:
            bm25_retriever.k = 20
            bm25_docs = await asyncio.to_thread(bm25_retriever.invoke, search_query)
        print(f"  â”” [Result] BM25 ê²€ìƒ‰ ê²°ê³¼: {len(bm25_docs)}ê°œ")
        
        # Step 3: Ensemble
        seen_contents = set()
        combined_docs = []
        for doc in bm25_docs + vector_docs:
            if doc.page_content not in seen_contents:
                seen_contents.add(doc.page_content)
                combined_docs.append(doc)
        
        # Step 4: Reranking
        final_docs = DirectSearchEngine.rerank_documents(search_query, combined_docs, top_n=5)
        print(f"  â”” [Result] ìµœì¢… ë¦¬ë­í‚¹ ê²°ê³¼: {len(final_docs)}ê°œ")

    except Exception as e:
        print(f"âŒ Search Pipeline Error: {e}")
        yield f"ê²€ìƒ‰ ì¤‘ ì‹œìŠ¤í…œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
        return

    if not final_docs:
        yield f"ğŸ” **'{display_query}'** ê´€ë ¨ ë¦¬í¬íŠ¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\n(DB ê²½ë¡œì™€ ì•± ì´ë¦„ì„ í™•ì¸í•´ì£¼ì„¸ìš”)"
        return

    # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    context_text = ""
    used_sources = []
    
    for i, doc in enumerate(final_docs, 1):
        meta = doc.metadata
        level = meta.get('an_level', 'Report')
        ver = meta.get('version', 'Unknown')
        source_name = f"[{meta.get('app_name')}] {level} (v{ver})"
        if meta.get('quarter_id'): source_name += f" - {meta.get('quarter_id')}"
        used_sources.append(source_name)
        context_text += f"\n[ë¬¸ì„œ {i}] ì¶œì²˜: {source_name} | ë‚´ìš©: {doc.page_content}"

    current_time = datetime.now().strftime("%Y-%m-%d %H:%M")
    
    prompt_template = """ë‹¹ì‹ ì€ ì•± ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ 'KIWI AI'ì…ë‹ˆë‹¤.
ì œê³µëœ [ì°¸ê³  ë¬¸ì„œ]ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•´ ì „ë¬¸ì ìœ¼ë¡œ ë‹µë³€í•˜ì‹­ì‹œì˜¤.

[ë¶„ì„ í™˜ê²½]
- ì‹œê°„: {current_time}
- ì•±: {target_apps_str}

[ì°¸ê³  ë¬¸ì„œ]
{context}

[ì§ˆë¬¸]
{query}

[ë‹µë³€ ê°€ì´ë“œ]
1. ë¬¸ì„œì— ìˆëŠ” ì‚¬ì‹¤(Data)ì— ê¸°ë°˜í•˜ì—¬ êµ¬ì²´ì ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
2. ë²„ê·¸ë‚˜ ë¬¸ì œì ì€ í•´ê²° ì—¬ë¶€ë‚˜ íŠ¹ì • ë²„ì „ì„ ì–¸ê¸‰í•˜ì„¸ìš”.
3. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³  ì—†ë‹¤ê³  ë§í•˜ì„¸ìš”.
4. ì¤‘ìš” í‚¤ì›Œë“œëŠ” **êµµê²Œ** í‘œì‹œí•˜ì„¸ìš”.
"""
    prompt = ChatPromptTemplate.from_template(prompt_template)
    chain = prompt | llm | StrOutputParser()

    try:
        async for chunk in chain.astream({
            "context": context_text,
            "query": display_query,
            "current_time": current_time,
            "target_apps_str": ", ".join(target_apps)
        }):
            yield chunk
        
        unique_sources = sorted(list(set(used_sources)))
        yield "\n\n---\n**ğŸ“š ì°¸ê³  ë¦¬í¬íŠ¸:**\n" + "\n".join([f"- {s}" for s in unique_sources])

    except Exception as e:
        print(f"âŒ Generation Error: {e}")
        yield "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."