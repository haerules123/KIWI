import mysql.connector
import asyncio
import os
import torch
import gc
import json
import re
from datetime import datetime
from tqdm import tqdm

from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.cross_encoders import HuggingFaceCrossEncoder
from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter
from keybert import KeyBERT
from sentence_transformers import SentenceTransformer
from konlpy.tag import Okt

from dotenv import load_dotenv 
load_dotenv()

# [ì„¤ì •]
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
PERSIST_DIRECTORY = "./chromadb_report"

# GPU ì„¤ì •
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"âš¡ í•˜ë“œì›¨ì–´ ê°€ì†: {device.upper()}")

# ì„ë² ë”© ëª¨ë¸ (ì „ì—­)
embeddings = HuggingFaceEmbeddings(
    model_name="BAAI/bge-m3",
    model_kwargs={'device': device},
    encode_kwargs={'normalize_embeddings': True, 'batch_size': 32}
)

# Reranker
reranker_model = HuggingFaceCrossEncoder(
    model_name="dragonkue/bge-reranker-v2-m3-ko",
    model_kwargs={'device': device}
)

# LLM ì„¤ì •
llm_drafter = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash", 
    temperature=0.3,
    google_api_key=GEMINI_API_KEY,
    timeout=60
)

# DB ì„¤ì •
DB_CONFIG = {
    'host': os.getenv('host'),
    'user': os.getenv('user'),
    'password': os.getenv('passwd'),
    'database': os.getenv('dbname')
}

class MetadataEnsemble:
    def __init__(self, shared_embeddings):
        """
        shared_embeddings: ì „ì—­ ì„ë² ë”© ëª¨ë¸ ì¬ì‚¬ìš©
        """
        self.okt = Okt()
        print("  â”” KeyBERT ì´ˆê¸°í™” ì¤‘...")
        # HuggingFaceEmbeddingsì˜ ë‚´ë¶€ ëª¨ë¸ì— ì ‘ê·¼
        try:
            # _client ì†ì„± ì‹œë„
            self.kw_model = KeyBERT(model=shared_embeddings._client)
        except AttributeError:
            # ì‹¤íŒ¨ ì‹œ ì§ì ‘ ë¡œë“œ
            print("    (ë³„ë„ SentenceTransformer ë¡œë“œ)")
            kw_sentence_model = SentenceTransformer("BAAI/bge-m3", device=device)
            self.kw_model = KeyBERT(model=kw_sentence_model)
    
    def _extract_keywords(self, text, top_n=15):
        """í‚¤ì›Œë“œ ì¶”ì¶œ (KeyBERT + Okt)"""
        try:
            nouns = " ".join(self.okt.nouns(text))
            if not nouns.strip():
                return []
            
            keywords = self.kw_model.extract_keywords(
                nouns, 
                keyphrase_ngram_range=(1, 2), 
                stop_words=None, 
                top_n=top_n
            )
            return [k[0] for k in keywords]
        except Exception as e:
            print(f"      âš ï¸ KeyBERT ì‹¤íŒ¨: {e}")
            return []

    async def generate_report_metadata(self, report_text, app_name, version):
        """
        ë³´ê³ ì„œ ì „ì²´(10000ì ì´í•˜)ì— ëŒ€í•´ LLM 1íšŒ í˜¸ì¶œ
        ì„±ê³µ ì‹œì—ë§Œ ë©”íƒ€ë°ì´í„° ë°˜í™˜, ì‹¤íŒ¨ ì‹œ None ë°˜í™˜
        """
        print(f"    ğŸ” KeyBERT í‚¤ì›Œë“œ ì¶”ì¶œ...", end=" ", flush=True)
        keywords = self._extract_keywords(report_text, top_n=15)
        print(f"{len(keywords)}ê°œ ì™„ë£Œ")
        
        # LLM í”„ë¡¬í”„íŠ¸ (ë³´ê³ ì„œ ì „ì²´ ì…ë ¥)
        llm_prompt = f"""ë‹¹ì‹ ì€ ì•± ë¦¬ë·° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¤ìŒì€ [{app_name}] v{version}ì˜ ì‚¬ìš©ì ë¦¬ë·° ë¶„ì„ ë³´ê³ ì„œ ì „ë¬¸ì…ë‹ˆë‹¤.

ë³´ê³ ì„œë¥¼ ë¶„ì„í•˜ì—¬ **ë°˜ë“œì‹œ ìœ íš¨í•œ JSON í˜•ì‹ìœ¼ë¡œë§Œ** ì‘ë‹µí•˜ì„¸ìš”.

[ë³´ê³ ì„œ]:
{report_text}

[ì¶œë ¥ í˜•ì‹ - JSONë§Œ ì¶œë ¥, ë‹¤ë¥¸ í…ìŠ¤íŠ¸ ì ˆëŒ€ ê¸ˆì§€]:
{{
    "sentiment": "ê¸ì •/ë¶€ì •/ì¤‘ë¦½",
    "features": ["ì£¼ìš”ê¸°ëŠ¥1", "ì£¼ìš”ê¸°ëŠ¥2", "ì£¼ìš”ê¸°ëŠ¥3"]
}}
"""
        
        print(f"    ğŸ¤– LLM ë©”íƒ€ë°ì´í„° ë¶„ì„...", end=" ", flush=True)
        try:
            # LLM í˜¸ì¶œ (íƒ€ì„ì•„ì›ƒ 60ì´ˆ)
            response = await asyncio.wait_for(
                llm_drafter.ainvoke(llm_prompt),
                timeout=60.0
            )
            
            # JSON íŒŒì‹±
            content = response.content.strip()
            json_match = re.search(r'\{.*\}', content, re.DOTALL)
            
            if not json_match:
                print("âŒ JSON í˜•ì‹ ì—†ìŒ")
                return None
            
            llm_data = json.loads(json_match.group())
            
            # í•„ìˆ˜ í•„ë“œ ê²€ì¦
            if "sentiment" not in llm_data or "features" not in llm_data:
                print("âŒ í•„ìˆ˜ í•„ë“œ ëˆ„ë½")
                return None
            
            # featuresê°€ ë¦¬ìŠ¤íŠ¸ì¸ì§€ í™•ì¸
            if not isinstance(llm_data["features"], list):
                print("âŒ featuresê°€ ë¦¬ìŠ¤íŠ¸ ì•„ë‹˜")
                return None
            
            print("âœ… ì™„ë£Œ")
            
            # ë°ì´í„° ì •ì œ
            sentiment = str(llm_data["sentiment"])
            features = [str(f) for f in llm_data["features"] if f]
            
            metadata = {
                "keywords": ", ".join(keywords[:15]),
                "sentiment": sentiment,
                "features": ", ".join(features[:5]) if features else "ì—†ìŒ"
            }
            
            return metadata
            
        except asyncio.TimeoutError:
            print("âŒ íƒ€ì„ì•„ì›ƒ")
            return None
        except json.JSONDecodeError as e:
            print(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {str(e)[:30]}")
            return None
        except Exception as e:
            print(f"âŒ LLM ì˜¤ë¥˜: {str(e)[:40]}")
            return None

async def fetch_new_reports_from_db():
    """ì•„ì§ ì²˜ë¦¬ë˜ì§€ ì•Šì€ ë³´ê³ ì„œ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
    conn = mysql.connector.connect(**DB_CONFIG)
    cursor = conn.cursor(dictionary=True)
    
    query = """
    SELECT 
        an.an_idx,
        a.a_name as app_name,
        v.v_version as version,
        an.an_text as report_markdown,
        MIN(r.r_date) as latest_review_date
    FROM analytics an
    JOIN version v ON an.v_idx = v.v_idx
    JOIN app a ON v.a_idx = a.a_idx
    JOIN review r ON v.v_idx = r.v_idx
    WHERE an.an_vectorized_at IS NULL
    GROUP BY an.an_idx;
    """
    
    cursor.execute(query)
    rows = cursor.fetchall()
    
    cursor.close()
    conn.close()
    return rows

def update_single_report_timestamp(an_idx):
    """DB íƒ€ì„ìŠ¤íƒ¬í”„ ì—…ë°ì´íŠ¸"""
    conn = mysql.connector.connect(**DB_CONFIG)
    cursor = conn.cursor()
    
    now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    
    try:
        query = "UPDATE analytics SET an_vectorized_at = %s WHERE an_idx = %s"
        cursor.execute(query, (now, an_idx))
        conn.commit()
    except Exception as e:
        print(f"      âŒ DB ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
        conn.rollback()
    finally:
        cursor.close()
        conn.close()

def clear_memory_cache():
    """ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬"""
    if device == "cuda":
        torch.cuda.empty_cache()
    gc.collect()

async def ingest_db_to_vector():
    db_reports = await fetch_new_reports_from_db()
    
    if not db_reports:
        print("ğŸ‰ ëª¨ë“  ë³´ê³ ì„œê°€ ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return

    print(f"ğŸ“¦ ì‹ ê·œ ë³´ê³ ì„œ {len(db_reports)}ê°œë¥¼ ìˆœì°¨ ì²˜ë¦¬í•©ë‹ˆë‹¤.")

    # ChromaDB ì´ˆê¸°í™”
    print("ğŸ”§ ChromaDB ì´ˆê¸°í™” ì¤‘...", end=" ", flush=True)
    vector_store = Chroma(
        embedding_function=embeddings,
        persist_directory=PERSIST_DIRECTORY
    )
    print("ì™„ë£Œ")

    # MetadataEnsemble ì´ˆê¸°í™”
    extractor = MetadataEnsemble(shared_embeddings=embeddings)
    
    # ìŠ¤í”Œë¦¬í„° ì„¤ì •
    md_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[
        ("#", "report_title"), ("##", "category"), ("###", "sub_category")
    ])
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)

    # í†µê³„
    success_count = 0
    failed_count = 0

    # ë³´ê³ ì„œ ì²˜ë¦¬ ë£¨í”„
    for idx, row in enumerate(tqdm(db_reports, desc="Processing Reports"), 1):
        an_idx = row['an_idx']
        
        try:
            print(f"\n  ğŸ“„ ë³´ê³ ì„œ ID {an_idx} ì²˜ë¦¬ ì‹œì‘...")
            
            # ë‚ ì§œ ì²˜ë¦¬
            dt = row['latest_review_date']
            if dt is None:
                print(f"    âš ï¸ ë‚ ì§œ ì •ë³´ ì—†ìŒ: ìŠ¤í‚µ")
                failed_count += 1
                continue
            
            year_int = int(dt.year)
            month_int = int(dt.month)
            quarter_int = (month_int - 1) // 3 + 1
            quarter_id = f"{year_int}-Q{quarter_int}"
            full_date = dt.strftime('%Y-%m-%d')

            # â˜…â˜…â˜… LLM ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (1íšŒ í˜¸ì¶œ) â˜…â˜…â˜…
            report_metadata = await extractor.generate_report_metadata(
                report_text=row['report_markdown'],
                app_name=row['app_name'],
                version=row['version']
            )
            
            # LLM ì‹¤íŒ¨ ì‹œ ë³´ê³ ì„œ ì „ì²´ ìŠ¤í‚µ
            if report_metadata is None:
                print(f"    âš ï¸ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨ â†’ ì´ ë³´ê³ ì„œëŠ” ê±´ë„ˆëœ€ (ë‹¤ìŒ ì‹¤í–‰ ì‹œ ì¬ì‹œë„)")
                failed_count += 1
                continue

            # í…ìŠ¤íŠ¸ ë¶„í• 
            print(f"    ğŸ”ª í…ìŠ¤íŠ¸ ë¶„í• ...", end=" ", flush=True)
            current_report_chunks = []
            header_splits = md_splitter.split_text(row['report_markdown'])
            
            chunk_count = 0
            for doc in header_splits:
                sub_chunks = text_splitter.split_documents([doc])
                for chunk in sub_chunks:
                    chunk_count += 1
                    
                    # ê¸°ë³¸ ë©”íƒ€ë°ì´í„°
                    chunk.metadata.update({
                        "source_an_idx": an_idx,
                        "app_name": row['app_name'],
                        "version": row['version'],
                        "year": year_int,
                        "month": month_int,
                        "quarter": quarter_int,
                        "quarter_id": quarter_id,
                        "date": full_date,
                        "doc_level": "version",
                        "genre": row.get('ag_name', 'Unknown')
                    })
                    
                    # LLM ë©”íƒ€ë°ì´í„° ì¶”ê°€
                    chunk.metadata.update(report_metadata)
                    
                    current_report_chunks.append(chunk)
            
            print(f"{chunk_count}ê°œ ì²­í¬ ìƒì„±")
            
            # ChromaDB ì €ì¥
            if current_report_chunks:
                print(f"    ğŸ’¾ ChromaDB ì €ì¥...", end=" ", flush=True)
                vector_store.add_documents(current_report_chunks)
                print("ì™„ë£Œ")
                
                print(f"    ğŸ—ƒï¸ DB íƒ€ì„ìŠ¤íƒ¬í”„ ì—…ë°ì´íŠ¸...", end=" ", flush=True)
                update_single_report_timestamp(an_idx)
                print("ì™„ë£Œ")
                
                print(f"  âœ… ID {an_idx} ì™„ë£Œ ({chunk_count}ê°œ ì²­í¬)")
                success_count += 1
            else:
                print(f"    âš ï¸ ì²­í¬ ì—†ìŒ")
                failed_count += 1

            # ë©”ëª¨ë¦¬ ì •ë¦¬ (20ê°œë§ˆë‹¤)
            if idx % 20 == 0:
                print(f"  ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬...")
                clear_memory_cache()
                print(f"  ğŸ“Š ì§„í–‰ ìƒí™©: ì„±ê³µ {success_count}ê°œ, ì‹¤íŒ¨ {failed_count}ê°œ")

        except Exception as e:
            print(f"\nâŒ ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬ (ID: {an_idx}): {e}")
            import traceback
            traceback.print_exc()
            failed_count += 1
            clear_memory_cache()
            continue

    # ìµœì¢… í†µê³„
    print("\n" + "="*60)
    print(f"âœ… ì²˜ë¦¬ ì™„ë£Œ: {success_count}ê°œ")
    print(f"âŒ ì‹¤íŒ¨/ìŠ¤í‚µ: {failed_count}ê°œ")
    print(f"ğŸ“Š ì „ì²´: {success_count + failed_count}ê°œ")
    print("="*60)
    
    if failed_count > 0:
        print(f"\nğŸ’¡ ì‹¤íŒ¨í•œ {failed_count}ê°œ ë³´ê³ ì„œëŠ” ë‹¤ìŒ ì‹¤í–‰ ì‹œ ì¬ì‹œë„ë©ë‹ˆë‹¤.")
    
    print("\nğŸ§¹ ìµœì¢… ë©”ëª¨ë¦¬ ì •ë¦¬...")
    clear_memory_cache()
    print("âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")

if __name__ == "__main__":
    try:
        asyncio.run(ingest_db_to_vector())
    except KeyboardInterrupt:
        print("\nğŸ›‘ ì‚¬ìš©ì ì¤‘ë‹¨")
        clear_memory_cache()
    except Exception as e:
        print(f"\nâŒ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        clear_memory_cache()