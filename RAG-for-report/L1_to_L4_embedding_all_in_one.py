import mysql.connector
import asyncio
import os
import torch
import gc
import json
import re
from datetime import datetime
from tqdm import tqdm

from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.cross_encoders import HuggingFaceCrossEncoder
from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter
from keybert import KeyBERT
from sentence_transformers import SentenceTransformer
from konlpy.tag import Okt

from dotenv import load_dotenv 
load_dotenv()

# [ì„¤ì •]
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
PERSIST_DIRECTORY = "./chromadb_report"

# GPU ì„¤ì •
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"âš¡ í•˜ë“œì›¨ì–´ ê°€ì†: {device.upper()}")

# ì„ë² ë”© ëª¨ë¸ (ì „ì—­)
embeddings = HuggingFaceEmbeddings(
    model_name="BAAI/bge-m3",
    model_kwargs={'device': device},
    encode_kwargs={'normalize_embeddings': True, 'batch_size': 32}
)

# LLM ì„¤ì •
llm_drafter = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash", 
    temperature=0.3,
    google_api_key=GEMINI_API_KEY,
    timeout=60
)

# DB ì„¤ì •
DB_CONFIG = {
    'host': os.getenv('host'),
    'user': os.getenv('user'),
    'password': os.getenv('passwd'),
    'database': os.getenv('dbname')
}

# (MetadataEnsemble í´ë˜ìŠ¤ëŠ” ê¸°ì¡´ê³¼ ë™ì¼í•˜ë¯€ë¡œ ìƒëµ ê°€ëŠ¥, ê·¸ëŒ€ë¡œ ë‘ì‹œë©´ ë©ë‹ˆë‹¤)
class MetadataEnsemble:
    def __init__(self, shared_embeddings):
        self.okt = Okt()
        print("  â”” KeyBERT ì´ˆê¸°í™” ì¤‘...")
        try:
            self.kw_model = KeyBERT(model=shared_embeddings._client)
        except AttributeError:
            kw_sentence_model = SentenceTransformer("BAAI/bge-m3", device=device)
            self.kw_model = KeyBERT(model=kw_sentence_model)
    
    def _extract_keywords(self, text, top_n=15):
        try:
            nouns = " ".join(self.okt.nouns(text))
            if not nouns.strip(): return []
            keywords = self.kw_model.extract_keywords(nouns, keyphrase_ngram_range=(1, 2), top_n=top_n)
            return [k[0] for k in keywords]
        except: return []

    async def generate_report_metadata(self, report_text, app_name, version):
        # (ê¸°ì¡´ ë¡œì§ ë™ì¼)
        keywords = self._extract_keywords(report_text, top_n=15)
        llm_prompt = f"""ë‹¹ì‹ ì€ ì•± ë¦¬ë·° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¤ìŒì€ [{app_name}] v{version}ì˜ ì‚¬ìš©ì ë¦¬ë·° ë¶„ì„ ë³´ê³ ì„œ ì „ë¬¸ì…ë‹ˆë‹¤.
ë³´ê³ ì„œë¥¼ ë¶„ì„í•˜ì—¬ **ë°˜ë“œì‹œ ìœ íš¨í•œ JSON í˜•ì‹ìœ¼ë¡œë§Œ** ì‘ë‹µí•˜ì„¸ìš”.
[ë³´ê³ ì„œ]: {report_text[:3000]}... (ìƒëµ)
[ì¶œë ¥ í˜•ì‹]: {{ "sentiment": "ê¸ì •/ë¶€ì •/ì¤‘ë¦½", "features": ["ê¸°ëŠ¥1", "ê¸°ëŠ¥2"] }}"""
        try:
            response = await asyncio.wait_for(llm_drafter.ainvoke(llm_prompt), timeout=60.0)
            content = response.content.strip()
            json_match = re.search(r'\{.*\}', content, re.DOTALL)
            if not json_match: return None
            llm_data = json.loads(json_match.group())
            return {
                "keywords": ", ".join(keywords[:15]),
                "sentiment": str(llm_data.get("sentiment", "ì¤‘ë¦½")),
                "features": ", ".join([str(f) for f in llm_data.get("features", [])][:5])
            }
        except: return None

# ==============================================================================
# ğŸš€ ìˆ˜ì •ëœ DB Fetch í•¨ìˆ˜ (L2~L4 í¬í•¨)
# ==============================================================================
async def fetch_new_reports_from_db():
    conn = mysql.connector.connect(**DB_CONFIG)
    cursor = conn.cursor(dictionary=True)
    
    # 1. review í…Œì´ë¸” ì¡°ì¸ì„ LEFT JOINìœ¼ë¡œ ë³€ê²½
    # 2. an_level ì¶”ê°€ ì¡°íšŒ
    query = """
    SELECT 
        an.an_idx,
        an.an_level,
        a.a_name as app_name,
        a.ag_idx,
        ag.ag_name,
        v.v_version as version,
        an.an_text as report_markdown,
        MIN(r.r_date) as latest_review_date
    FROM analytics an
    JOIN version v ON an.v_idx = v.v_idx
    JOIN app a ON v.a_idx = a.a_idx
    JOIN app_genre ag ON a.ag_idx = ag.ag_idx
    LEFT JOIN review r ON v.v_idx = r.v_idx  -- ğŸ”¥ LEFT JOINìœ¼ë¡œ ë³€ê²½
    WHERE an.an_vectorized_at IS NULL
    GROUP BY an.an_idx;
    """
    
    cursor.execute(query)
    rows = cursor.fetchall()
    cursor.close()
    conn.close()
    return rows

def update_single_report_timestamp(an_idx):
    conn = mysql.connector.connect(**DB_CONFIG)
    cursor = conn.cursor()
    now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    try:
        cursor.execute("UPDATE analytics SET an_vectorized_at = %s WHERE an_idx = %s", (now, an_idx))
        conn.commit()
    except: conn.rollback()
    finally:
        cursor.close()
        conn.close()

def clear_memory_cache():
    if device == "cuda": torch.cuda.empty_cache()
    gc.collect()

# ==============================================================================
# ğŸš€ ë©”ì¸ ì²˜ë¦¬ ë¡œì§ (ë‚ ì§œ íŒŒì‹± ë¡œì§ ì¶”ê°€)
# ==============================================================================
async def ingest_db_to_vector():
    db_reports = await fetch_new_reports_from_db()
    
    if not db_reports:
        print("ğŸ‰ ëª¨ë“  ë³´ê³ ì„œê°€ ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return

    print(f"ğŸ“¦ ì‹ ê·œ ë³´ê³ ì„œ {len(db_reports)}ê°œë¥¼ ìˆœì°¨ ì²˜ë¦¬í•©ë‹ˆë‹¤.")

    vector_store = Chroma(
        embedding_function=embeddings,
        persist_directory=PERSIST_DIRECTORY
    )
    extractor = MetadataEnsemble(shared_embeddings=embeddings)
    
    md_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[
        ("#", "report_title"), ("##", "category"), ("###", "sub_category")
    ])
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)

    success_count = 0
    failed_count = 0

    for idx, row in enumerate(tqdm(db_reports, desc="Processing Reports"), 1):
        an_idx = row['an_idx']
        an_level = row['an_level']
        version_str = row['version']
        
        try:
            print(f"\n  ğŸ“„ [{an_level}] ID {an_idx} ({row['app_name']} v{version_str}) ì²˜ë¦¬ ì¤‘...")
            
            # ğŸ”¥ [í•µì‹¬ ìˆ˜ì •] ë‚ ì§œ ë° ë©”íƒ€ë°ì´í„° ê³„ì‚° ë¡œì§
            meta_year = 0
            meta_month = 0
            meta_quarter = 0
            meta_quarter_id = "Unknown"
            meta_date = "Unknown"

            # Case 1: L1 (ì‹¤ì œ ë¦¬ë·° ë‚ ì§œ ê¸°ë°˜)
            if an_level == 'L1':
                dt = row['latest_review_date']
                if dt:
                    meta_year = int(dt.year)
                    meta_month = int(dt.month)
                    meta_quarter = (meta_month - 1) // 3 + 1
                    meta_quarter_id = f"{meta_year}-Q{meta_quarter}"
                    meta_date = dt.strftime('%Y-%m-%d')
                else:
                    # L1ì¸ë° ë‚ ì§œê°€ ì—†ìœ¼ë©´ ë¬¸ì œ
                    print("    âš ï¸ L1ì¸ë° ë‚ ì§œ ì •ë³´ ì—†ìŒ (SKIP)")
                    failed_count += 1
                    continue

            # Case 2: L2 (ë¶„ê¸° ë¦¬í¬íŠ¸, ì˜ˆ: '2024-Q1')
            elif an_level == 'L2':
                try:
                    # version_str = '2024-Q1'
                    y_str, q_str = version_str.split('-Q')
                    meta_year = int(y_str)
                    meta_quarter = int(q_str)
                    meta_month = (meta_quarter - 1) * 3 + 1
                    meta_quarter_id = version_str
                    meta_date = f"{meta_year}-{meta_month:02d}-01" # í•´ë‹¹ ë¶„ê¸° ì²«ë‚ ë¡œ ì„¤ì •
                except:
                    print(f"    âš ï¸ L2 ë²„ì „ í˜•ì‹ ì˜¤ë¥˜ ({version_str})")
                    failed_count += 1
                    continue

            # Case 3: L3 (ì—°ê°„ ë¦¬í¬íŠ¸, ì˜ˆ: '2024')
            elif an_level == 'L3':
                try:
                    meta_year = int(version_str)
                    meta_quarter = 0 # ì—°ê°„ì€ ë¶„ê¸° ì—†ìŒ
                    meta_quarter_id = f"{meta_year}-ALL"
                    meta_date = f"{meta_year}-01-01"
                except:
                    print(f"    âš ï¸ L3 ë²„ì „ í˜•ì‹ ì˜¤ë¥˜ ({version_str})")
                    failed_count += 1
                    continue

            # Case 4: L4 (ì¢…í•© ë¦¬í¬íŠ¸, ì˜ˆ: 'TOTAL')
            elif an_level == 'L4':
                meta_year = 9999
                meta_quarter_id = "TOTAL"
                meta_date = datetime.now().strftime('%Y-%m-%d') # ì²˜ë¦¬ ì‹œì  ë‚ ì§œ

            # LLM ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            report_metadata = await extractor.generate_report_metadata(
                report_text=row['report_markdown'],
                app_name=row['app_name'],
                version=version_str
            )
            
            if report_metadata is None:
                print(f"    âš ï¸ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨ (SKIP)")
                failed_count += 1
                continue

            # í…ìŠ¤íŠ¸ ë¶„í•  ë° ì²­í¬ ìƒì„±
            current_report_chunks = []
            header_splits = md_splitter.split_text(row['report_markdown'])
            
            for doc in header_splits:
                sub_chunks = text_splitter.split_documents([doc])
                for chunk in sub_chunks:
                    # ê³µí†µ ë©”íƒ€ë°ì´í„°
                    chunk.metadata.update({
                        "source_an_idx": an_idx,
                        "app_name": row['app_name'],
                        "version": version_str,
                        "an_level": an_level,  # ë ˆë²¨ ì •ë³´ ì¶”ê°€
                        "year": meta_year,
                        "month": meta_month,
                        "quarter": meta_quarter,
                        "quarter_id": meta_quarter_id,
                        "date": meta_date,
                        "doc_level": an_level, # doc_levelë„ an_levelë¡œ ë§ì¶¤
                        "genre": row.get('ag_name', 'Unknown')
                    })
                    # LLM ë©”íƒ€ë°ì´í„° ë³‘í•©
                    chunk.metadata.update(report_metadata)
                    current_report_chunks.append(chunk)
            
            if current_report_chunks:
                vector_store.add_documents(current_report_chunks)
                update_single_report_timestamp(an_idx)
                print(f"  âœ… ì™„ë£Œ ({len(current_report_chunks)} ì²­í¬)")
                success_count += 1
            else:
                print("    âš ï¸ ìƒì„±ëœ ì²­í¬ ì—†ìŒ")
                failed_count += 1

            if idx % 20 == 0: clear_memory_cache()

        except Exception as e:
            print(f"\nâŒ ì—ëŸ¬ (ID: {an_idx}): {e}")
            failed_count += 1
            continue

    print("\n" + "="*60)
    print(f"âœ… ì²˜ë¦¬ ì™„ë£Œ: {success_count}ê°œ")
    print(f"âŒ ì‹¤íŒ¨/ìŠ¤í‚µ: {failed_count}ê°œ")
    print("="*60)
    clear_memory_cache()

if __name__ == "__main__":
    asyncio.run(ingest_db_to_vector())