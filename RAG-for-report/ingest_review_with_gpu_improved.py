import mysql.connector
import asyncio
import os
import torch
import re
import gc
from datetime import datetime
from langchain_huggingface import HuggingFaceEmbeddings  # Ollama ëŒ€ì‹  ì‚¬ìš©
from langchain_chroma import Chroma
from konlpy.tag import Okt
from keybert import KeyBERT
from sentence_transformers import SentenceTransformer
from tqdm import tqdm
from dotenv import load_dotenv
import numpy as np
import sys
import traceback
from langchain_core.documents import Document

load_dotenv()

# [ì„¤ì •]
PERSIST_DIRECTORY = "./chromadb_review_line"
EMBEDDING_MODEL = "BAAI/bge-m3"  # HuggingFace ëª¨ë¸ (ì•ˆì •ì„±)
BATCH_SIZE = 500  # ì•ˆì „ì„± ìš°ì„ 

# GPU ì„¤ì •
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"âš¡ í•˜ë“œì›¨ì–´ ê°€ì†: {device.upper()}")

DB_CONFIG = {
    'host': os.getenv('host'),
    'user': os.getenv('user'),
    'password': os.getenv('passwd'),
    'database': os.getenv('dbname')
}

class KeywordExtractor:
    def __init__(self):
        self.okt = Okt()
        print("  â”” KeyBERT ëª¨ë¸ ë¡œë“œ ì¤‘...")
        # bge-m3ì™€ ë™ì¼ ëª¨ë¸ ì‚¬ìš© (ì¼ê´€ì„±)
        sentence_model = SentenceTransformer('BAAI/bge-m3', device=device)
        self.kw_model = KeyBERT(model=sentence_model)

    def extract(self, text):
        if len(text) < 20:
            nouns = self.okt.nouns(text)
            return list(set(nouns))[:5]
        try:
            keywords = self.kw_model.extract_keywords(
                text, keyphrase_ngram_range=(1, 2), 
                stop_words=None, top_n=5
            )
            return [k[0] for k in keywords]
        except:
            return self.okt.nouns(text)[:5]

def clean_text(text):
    """í…ìŠ¤íŠ¸ ì •ì œ + ê²€ì¦"""
    if not text or not isinstance(text, str):
        return None
    
    # Null Byte ì œê±°
    text = text.replace('\x00', '')
    
    # ì œì–´ ë¬¸ì ì œê±°
    text = re.sub(r'[\x00-\x1F\x7F-\x9F]', '', text)
    
    # ê³µë°± ì •ê·œí™”
    text = ' '.join(text.split())
    
    # ìµœì†Œ ê¸¸ì´ ì²´í¬
    if len(text.strip()) < 5:
        return None
    
    return text.strip()

def validate_embedding_input(text):
    """ì„ë² ë”© ì…ë ¥ ìœ íš¨ì„± ê²€ì‚¬"""
    if not text:
        return False
    
    # ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ (ëª¨ë¸ í•œê³„ ì´ˆê³¼)
    if len(text) > 8000:
        return False
    
    # íŠ¹ìˆ˜ë¬¸ìë§Œ ìˆëŠ”ì§€ ì²´í¬
    if not re.search(r'[ê°€-í£a-zA-Z0-9]', text):
        return False
    
    return True

def fetch_unprocessed_batch(batch_size=500):
    conn = mysql.connector.connect(**DB_CONFIG)
    cursor = conn.cursor(dictionary=True)
    
    query = """
    SELECT 
        rl.rl_idx,
        rl.rl_line as text,
        an.a_idx,
        at.at_type as aspect,
        et.et_type as sentiment,
        v.v_version as version,
        a.a_name as app_name,
        ag.ag_name as app_genre,
        r.r_date as date
    FROM review_line rl
    JOIN analysis an ON rl.rl_idx = an.rl_idx
    JOIN aspect_type at ON an.at_idx = at.at_idx
    JOIN emotion_type et ON an.et_idx = et.et_idx
    JOIN review r ON rl.r_idx = r.r_idx
    JOIN version v ON r.v_idx = v.v_idx
    JOIN app a ON v.a_idx = a.a_idx
    JOIN app_genre ag ON a.ag_idx = ag.ag_idx
    WHERE rl.rl_vectorized_at IS NULL
    LIMIT %s;
    """
    
    cursor.execute(query, (batch_size,))
    rows = cursor.fetchall()
    
    cursor.close()
    conn.close()
    return rows

def mark_as_vectorized(rl_ids):
    """ì„±ê³µí•œ IDë§Œ ë§ˆí‚¹"""
    if not rl_ids:
        return

    conn = mysql.connector.connect(**DB_CONFIG)
    cursor = conn.cursor()
    
    now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    unique_ids = list(set(rl_ids))
    format_strings = ','.join(['%s'] * len(unique_ids))
    
    query = f"UPDATE review_line SET rl_vectorized_at = %s WHERE rl_idx IN ({format_strings})"
    
    try:
        params = [now] + unique_ids
        cursor.execute(query, params)
        conn.commit()
        print(f"    âœ“ DB ì—…ë°ì´íŠ¸ ì™„ë£Œ: {len(unique_ids)}ê°œ")
    except Exception as e:
        print(f"    âŒ DB ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
        conn.rollback()
    finally:
        cursor.close()
        conn.close()

def mark_as_failed(rl_ids):
    """ì‹¤íŒ¨í•œ IDëŠ” íŠ¹ìˆ˜ ê°’ìœ¼ë¡œ ë§ˆí‚¹ (ì¬ì²˜ë¦¬ ë°©ì§€)"""
    if not rl_ids:
        return

    conn = mysql.connector.connect(**DB_CONFIG)
    cursor = conn.cursor()
    
    failed_timestamp = '1970-01-01 00:00:00'  # ì‹¤íŒ¨ í‘œì‹œ
    unique_ids = list(set(rl_ids))
    format_strings = ','.join(['%s'] * len(unique_ids))
    
    query = f"UPDATE review_line SET rl_vectorized_at = %s WHERE rl_idx IN ({format_strings})"
    
    try:
        params = [failed_timestamp] + unique_ids
        cursor.execute(query, params)
        conn.commit()
        print(f"    âš ï¸ ì‹¤íŒ¨ ë°ì´í„° ë§ˆí‚¹: {len(unique_ids)}ê°œ")
    except Exception as e:
        print(f"    âŒ ì‹¤íŒ¨ ë§ˆí‚¹ ì¤‘ ì˜¤ë¥˜: {e}")
        conn.rollback()
    finally:
        cursor.close()
        conn.close()

async def ingest_reviews():
    print("ğŸš€ ë¦¬ë·° ë°ì´í„° ë²¡í„°í™” íŒŒì´í”„ë¼ì¸ ì‹œì‘...")
    print(f"âš™ï¸  ë°°ì¹˜ ì‚¬ì´ì¦ˆ: {BATCH_SIZE}ê°œ")
    
    # HuggingFace Embeddings (Ollamaë³´ë‹¤ ì•ˆì •ì )
    embeddings = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL,
        model_kwargs={'device': device},
        encode_kwargs={
            'normalize_embeddings': True,
            'batch_size': 32
        }
    )
    
    vector_store = Chroma(
        embedding_function=embeddings,
        persist_directory=PERSIST_DIRECTORY,
        collection_name="review_sentences"
    )
    
    extractor = KeywordExtractor()
    total_processed = 0
    total_failed = 0

    while True:
        reviews = fetch_unprocessed_batch(BATCH_SIZE)
        
        if not reviews:
            print(f"\nğŸ‰ ì™„ë£Œ! ì´ ì²˜ë¦¬: {total_processed}ê°œ | ì‹¤íŒ¨: {total_failed}ê°œ")
            break
            
        print(f"\nğŸ“¦ ë°°ì¹˜ ì²˜ë¦¬: {len(reviews)}ê°œ")
        
        documents = []
        successful_ids = []
        failed_ids = []

        for row in tqdm(reviews, desc="  ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"):
            raw_text = row['text']
            rl_idx = row['rl_idx']
            
            # 1. í…ìŠ¤íŠ¸ ì •ì œ
            text = clean_text(raw_text)
            
            if not text or not validate_embedding_input(text):
                failed_ids.append(rl_idx)
                continue

            # 2. í‚¤ì›Œë“œ ì¶”ì¶œ (ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰)
            try:
                keywords = extractor.extract(text)
            except:
                keywords = []
            
            # 3. ë©”íƒ€ë°ì´í„° êµ¬ì„±
            metadata = {
                "rl_idx": rl_idx,
                "a_idx": row['a_idx'],
                "app_name": row['app_name'],
                "app_genre": row['app_genre'],
                "version": row['version'],
                "aspect": row['aspect'],
                "sentiment": row['sentiment'],
                "date": str(row['date']),
                "keywords": ", ".join(keywords) if keywords else ""
            }
            
            doc = Document(page_content=text, metadata=metadata)
            documents.append(doc)
            successful_ids.append(rl_idx)

        # 4. ë²¡í„° ì €ì¥ (ë°°ì¹˜ â†’ ê°œë³„ fallback)
        if documents:
            print(f"  ğŸ’¾ ë²¡í„°í™” ì¤‘: {len(documents)}ê°œ ë¬¸ì„œ...")
            
            try:
                # ë°°ì¹˜ ì €ì¥ ì‹œë„
                vector_store.add_documents(documents)
                mark_as_vectorized(successful_ids)
                total_processed += len(documents)
                
            except Exception as e:
                print(f"  âš ï¸ ë°°ì¹˜ ì €ì¥ ì‹¤íŒ¨: {e}")
                print(f"  ğŸ”„ ê°œë³„ ì €ì¥ ëª¨ë“œë¡œ ì „í™˜...")
                
                # ê°œë³„ ì €ì¥
                individual_success = []
                individual_fail = []
                
                for idx, doc in enumerate(tqdm(documents, desc="    ê°œë³„ ì €ì¥")):
                    try:
                        vector_store.add_documents([doc])
                        individual_success.append(successful_ids[idx])
                    except Exception as inner_e:
                        print(f"      âŒ ì‹¤íŒ¨ (ID: {successful_ids[idx]}): {str(inner_e)[:50]}")
                        individual_fail.append(successful_ids[idx])
                
                # ì„±ê³µ/ì‹¤íŒ¨ ë¶„ë¦¬ ë§ˆí‚¹
                if individual_success:
                    mark_as_vectorized(individual_success)
                    total_processed += len(individual_success)
                
                if individual_fail:
                    mark_as_failed(individual_fail)
                    total_failed += len(individual_fail)
        
        # 5. ë¶ˆëŸ‰ ë°ì´í„° ë§ˆí‚¹
        if failed_ids:
            mark_as_failed(failed_ids)
            total_failed += len(failed_ids)
        
        # 6. GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        if device == "cuda":
            torch.cuda.empty_cache()
        gc.collect()
        
        print(f"  ğŸ“Š ëˆ„ì  | ì„±ê³µ: {total_processed} | ì‹¤íŒ¨: {total_failed}")

# main í•¨ìˆ˜ë¥¼ try-exceptë¡œ ê°ì‹¸ê¸°
if __name__ == "__main__":
    try:
        asyncio.run(ingest_reviews())
    except Exception as e:
        print(f"\nâŒ ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ:")
        print(f"   ì—ëŸ¬ íƒ€ì…: {type(e).__name__}")
        print(f"   ë©”ì‹œì§€: {str(e)}")
        print(f"\nğŸ“ ìƒì„¸ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:")
        traceback.print_exc()
        sys.exit(1)