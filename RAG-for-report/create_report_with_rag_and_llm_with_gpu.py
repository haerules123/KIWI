import os
import sys
import gc
import ctypes
import json
import time
import warnings
from datetime import datetime
from dotenv import load_dotenv
from tqdm import tqdm
import mysql.connector
from mysql.connector.errors import OperationalError
import torch
import google.generativeai as genai

# ==============================================================================
# üõë [CRITICAL] ÏãúÏä§ÌÖú ÏÑ§Ï†ï (Î≥ÄÍ≤Ω ÏóÜÏùå)
# ==============================================================================
os.environ["PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION"] = "python"
os.environ["GRPC_POLL_STRATEGY"] = "epoll1"
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["HF_HUB_DISABLE_TELEMETRY"] = "1"
os.environ["TRANSFORMERS_OFFLINE"] = "0"
os.environ["OMP_NUM_THREADS"] = "1"
os.environ["MKL_NUM_THREADS"] = "1"
os.environ["OPENBLAS_NUM_THREADS"] = "1"
os.environ["VECLIB_MAXIMUM_THREADS"] = "1"
os.environ["NUMEXPR_NUM_THREADS"] = "1"
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
os.environ["OMP_WAIT_POLICY"] = "PASSIVE"
os.environ["MKL_THREADING_LAYER"] = "GNU"
os.environ["DUCKDB_THREADS"] = "1"
os.environ["CHROMA_OTEL_COLLECTION_ENDPOINT"] = ""

try:
    __import__('pysqlite3')
    sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
except ImportError:
    pass

# ==============================================================================

from utils.logger import setup_logger

# Logger Setup
logger = setup_logger("rag_report_gpu")

warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)

# torch ÏÑ§Ï†ï
torch.set_num_threads(1)
torch.set_num_interop_threads(1)

from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from sentence_transformers import CrossEncoder
from langchain_core.documents import Document

load_dotenv()

# [‚öôÔ∏è ÏÑ§Ï†ïÍ∞í]
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
PERSIST_DIRECTORY = "./chromadb_review_line"
EMBEDDING_MODEL = "BAAI/bge-m3"
RERANKER_MODEL_NAME = "Dongjin-kr/ko-reranker"

TARGET_GENRE = "ÏóîÌÑ∞ÌÖåÏù∏Î®ºÌä∏"  
MIN_REVIEW_LINES = 10  # SQL ÌïÑÌÑ∞ÎßÅ Í∏∞Ï§Ä

MAX_BATCH_SIZE = 2
RERANK_BATCH_LIMIT = 10
SLEEP_BETWEEN_BATCH = 3
SLEEP_AFTER_ERROR = 5

DB_CONFIG = {
    'host': os.getenv('host'),
    'user': os.getenv('user'),
    'password': os.getenv('passwd'),
    'database': os.getenv('dbname'),
    'autocommit': False,
    'connection_timeout': 10,
    'pool_size': 1,
    'pool_reset_session': True
}

# Ï†ÑÏó≠ Î≥ÄÏàò
device = "cpu"
reranker = None
vector_store = None
embeddings = None

def initialize_runtime():
    global device
    device = "cuda" if torch.cuda.is_available() else "cpu"
    logger.info(f"üñ•Ô∏è [SYSTEM] Ïã§Ìñâ Ïû•Ïπò: {device.upper()}")
    try:
        if device == "cuda":
            try:
                print(f"   ‚îî GPU: {torch.cuda.get_device_name(0)}")
            except Exception:
                pass

        logger.info(f"üìÇ [DEBUG] Vector DB: {os.path.abspath(PERSIST_DIRECTORY)}")
        if not os.path.exists(PERSIST_DIRECTORY):
            logger.critical("üö® [CRITICAL] DB Ìè¥ÎçîÍ∞Ä ÏóÜÏäµÎãàÎã§!")
            return False

        try:
            genai.configure(api_key=GEMINI_API_KEY)
        except Exception:
            pass

        return True
    except Exception as e:
        logger.error(f"‚ùå Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
        return False

def get_db_connection():
    max_retries = 5
    retry_delay = 2
    for attempt in range(max_retries):
        try:
            conn = mysql.connector.connect(**DB_CONFIG)
            return conn
        except (OperationalError, Exception) as e:
            logger.warning(f"‚ö†Ô∏è DB Ïó∞Í≤∞ Ïã§Ìå® ({attempt+1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                time.sleep(retry_delay)
                retry_delay *= 2
            else:
                logger.error(f"‚ùå DB Ïó∞Í≤∞ ÏµúÏ¢Ö Ïã§Ìå®: {e}")
                return None

def close_db_safely(conn, cursor=None):
    try:
        if cursor: cursor.close()
    except: pass
    try:
        if conn and conn.is_connected(): conn.close()
    except: pass

# ==============================================================================
# üöÄ [OPTIMIZED] ÏµúÏ†ÅÌôîÎêú SQL ÏøºÎ¶¨ Ìï®Ïàò
# ==============================================================================
def get_target_versions(genre, min_lines):
    conn = get_db_connection()
    if not conn:
        return []
    cursor = None
    try:
        cursor = conn.cursor(dictionary=True)
        # 1. analytics ÌÖåÏù¥Î∏îÍ≥º JOINÌïòÏó¨ Ïù¥ÎØ∏ Ï°¥Ïû¨ÌïòÎäî Î¶¨Ìè¨Ìä∏ Ï†úÏô∏ (an.v_idx IS NULL)
        # 2. review_line ÏàòÎ•º Ïπ¥Ïö¥Ìä∏ÌïòÏó¨ min_lines Ïù¥ÏÉÅÏù∏ Í≤ÉÎßå Í∞ÄÏ†∏Ïò¥ (HAVING Ï†à)
        # 3. ÎÇ†Ïßú ÌïÑÌÑ∞ÎßÅÏù¥ ÌïÑÏöîÌïòÎã§Î©¥ WHERE Ï†àÏùò Ï£ºÏÑùÏùÑ Ìï¥Ï†úÌïòÍ≥† Ïã§Ï†ú Ïª¨ÎüºÎ™Ö ÏÇ¨Ïö©
        query = """
            SELECT v.v_idx, v.v_version, a.a_name, ag.ag_name, COUNT(rl.rl_idx) as line_count
            FROM version v
            JOIN app a ON v.a_idx = a.a_idx
            JOIN app_genre ag ON a.ag_idx = ag.ag_idx
            LEFT JOIN analytics an ON v.v_idx = an.v_idx
            JOIN review r ON v.v_idx = r.v_idx
            JOIN review_line rl ON r.r_idx = rl.r_idx
            WHERE ag.ag_name = %s
              AND an.v_idx IS NULL
              -- AND v.created_at >= '2023-01-01'  -- ‚ö†Ô∏è [ÎÇ†Ïßú ÌïÑÌÑ∞] Ïã§Ï†ú DB Ïª¨ÎüºÎ™Ö ÌôïÏù∏ ÌõÑ Ï£ºÏÑù Ìï¥Ï†ú
            GROUP BY v.v_idx, v.v_version, a.a_name, ag.ag_name
            HAVING COUNT(rl.rl_idx) >= %s
            ORDER BY a.a_name ASC, v.v_idx DESC
        """
        cursor.execute(query, (genre, min_lines))
        results = cursor.fetchall()
        return results
    except Exception as e:
        print(f"‚ùå Î≤ÑÏ†Ñ Ï°∞Ìöå Ïã§Ìå®: {e}")
        return []
    finally:
        close_db_safely(conn, cursor)

def save_report_to_db(v_idx, report_text):
    conn = get_db_connection()
    if not conn:
        return False
    cursor = None
    try:
        cursor = conn.cursor()
        query = "INSERT INTO analytics (an_text, an_vectorized_at, v_idx) VALUES (%s, NULL, %s)"
        cursor.execute(query, (report_text, v_idx))
        conn.commit()
        return True
    except Exception as e:
        print(f"‚ùå DB Ï†ÄÏû• Ïã§Ìå®: {e}")
        if conn:
            try: conn.rollback()
            except: pass
        return False
    finally:
        close_db_safely(conn, cursor)

def get_version_statistics(app_name, version):
    conn = get_db_connection()
    if not conn:
        return None, []
    cursor = None
    try:
        cursor = conn.cursor(dictionary=True)
        
        cursor.execute("""
            SELECT COUNT(DISTINCT r.r_idx) as total, AVG(r.r_score) as avg_rating
            FROM review r
            JOIN version v ON r.v_idx = v.v_idx
            JOIN app a ON v.a_idx = a.a_idx
            WHERE a.a_name = %s AND v.v_version = %s
        """, (app_name, version))
        base_stats = cursor.fetchone()
        
        cursor.execute("""
            SELECT 
                at.at_type as aspect,
                SUM(CASE WHEN et.et_type = 'Í∏çÏ†ï' THEN 1 ELSE 0 END) as pos_count,
                SUM(CASE WHEN et.et_type = 'Î∂ÄÏ†ï' THEN 1 ELSE 0 END) as neg_count,
                COUNT(*) as total_count
            FROM review_line rl
            JOIN analysis an ON rl.rl_idx = an.rl_idx
            JOIN aspect_type at ON an.at_idx = at.at_idx
            JOIN emotion_type et ON an.et_idx = et.et_idx
            JOIN review r ON rl.r_idx = r.r_idx
            JOIN version v ON r.v_idx = v.v_idx
            JOIN app a ON v.a_idx = a.a_idx
            WHERE a.a_name = %s AND v.v_version = %s
            GROUP BY at.at_type
            ORDER BY total_count DESC
        """, (app_name, version))
        aspect_stats = cursor.fetchall()
        return base_stats, aspect_stats
    except Exception as e:
        print(f"‚ùå ÌÜµÍ≥Ñ Ï°∞Ìöå Ïã§Ìå®: {e}")
        return None, []
    finally:
        close_db_safely(conn, cursor)

def get_rag_contexts(app_name, version, aspect, sentiment, global_used_texts, 
                     vector_store_instance, reranker_instance, top_k=15):
    query_text = f"{app_name} {aspect} {sentiment}"
    results = []

    for attempt in range(3):
        try:
            results = vector_store_instance.similarity_search(
                query=query_text, k=20,
                filter={"$and": [{"app_name": app_name}, {"version": version}]}
            )
            break
        except Exception as e:
            if attempt == 2: return []
            time.sleep(1)

    if not results: return []

    doc_texts = [doc.page_content for doc in results]
    valid_indices = [i for i, t in enumerate(doc_texts) if len(t) > 2]
    filtered_results = [results[i] for i in valid_indices]
    filtered_texts = [doc_texts[i] for i in valid_indices]

    if len(filtered_texts) > RERANK_BATCH_LIMIT:
        filtered_results = filtered_results[:RERANK_BATCH_LIMIT]
        filtered_texts = filtered_texts[:RERANK_BATCH_LIMIT]

    if not filtered_texts: return []

    rerank_query = f"{aspect} Í¥ÄÎ†® {sentiment} ÏùòÍ≤¨"
    pairs = [[rerank_query, text] for text in filtered_texts]
    
    if reranker_instance:
        try:
            scores = reranker_instance.predict(pairs, batch_size=1)
            if device == "cuda": torch.cuda.empty_cache()
        except Exception:
            scores = [0.0] * len(pairs)
    else:
        scores = [0.0] * len(pairs)
    
    scored_docs = sorted(list(zip(filtered_results, scores)), key=lambda x: x[1], reverse=True)
    final_data = []
    
    for doc, score in scored_docs:
        if len(final_data) >= top_k: break
        if score < -4.0: continue
        text = doc.page_content
        if text not in global_used_texts:
            final_data.append({
                "text": text, 
                "date": doc.metadata.get('date', 'Unknown'), 
                "relevance_score": float(score)
            })
            global_used_texts.add(text)
    
    del pairs, scores, scored_docs, results, filtered_results
    gc.collect()
    return final_data

def generate_ai_report(app_name, version, json_data, total_reviews, avg_rating):
    context_str = json.dumps(json_data, ensure_ascii=False, indent=2)
    prompt = f"""
ÎãπÏã†ÏùÄ Î™®Î∞îÏùº Ïï± QA ÏàòÏÑù Ïª®ÏÑ§ÌÑ¥Ìä∏ÏûÖÎãàÎã§. ÏïÑÎûò Îç∞Ïù¥ÌÑ∞Î•º Î∂ÑÏÑùÌïòÏó¨ Î≥¥Í≥†ÏÑúÎ•º ÏûëÏÑ±ÌïòÏã≠ÏãúÏò§.

üõë **ÏûëÏÑ± ÏõêÏπô**
1. **Format Ï§ÄÏàò**: ÏïÑÎûò Markdown ÌòïÏãùÏùÑ Í∑∏ÎåÄÎ°ú ÏÇ¨Ïö©.
2. **Ï∞ΩÏûë Í∏àÏßÄ**: 'Ïú†Ï†ÄÏùò Î™©ÏÜåÎ¶¨'Îäî JSONÏùò text, dateÎ•º Í∑∏ÎåÄÎ°ú Ïù∏Ïö©.
3. **Îç∞Ïù¥ÌÑ∞ Îß§Ìïë**: negative_reviewsÎäî ÏÑπÏÖò 2, positive_reviewsÎäî ÏÑπÏÖò 3Ïóê ÏÇ¨Ïö©.

---
[CONTEXT JSON]
{context_str}

---
[OUTPUT FORMAT]
# üì± [{app_name}] v{version} Ïã¨Ï∏µ Î∂ÑÏÑù Î≥¥Í≥†ÏÑú

## üìë Î≥¥Í≥†ÏÑú Í∞úÏöî
| Ìï≠Î™© | ÎÇ¥Ïö© |
| :--- | :--- |
| **Î∂ÑÏÑù ÎåÄÏÉÅ Î≤ÑÏ†Ñ** | {version} |
| **ÏÇ¨Ïö©Ïûê ÌèâÏ†ê** | {avg_rating:.2f} |
| **Î∂ÑÏÑù ÌëúÎ≥∏ Ïàò** | {total_reviews} Í∞úÏùò Ïú†Ìö® Î¶¨Î∑∞ |

---

## 1. üìä Executive Summary
- **Ï¢ÖÌï© Ï†êÏàò**: (ÌèâÏ†ê {avg_rating:.2f} Í∏∞Ï§Ä ÌåêÏ†ï)
- **ÌïµÏã¨ ÏöîÏïΩ**: (3Î¨∏Ïû• ÎÇ¥Ïô∏ ÏöîÏïΩ)
- **Í∏¥Í∏â ÎåÄÏùë Í≥ºÏ†ú**: (1Ï§Ñ ÏöîÏïΩ)

## 2. üö® Ïù¥Ïäà Ïã¨Ï∏µ Î∂ÑÏÑù (Deep Dive)
### 2.1 [Aspect Ïù¥Î¶Ñ] (Î∂ÄÏ†ï [neg_ratio])
**üí¨ Ïú†Ï†ÄÏùò Î™©ÏÜåÎ¶¨ (Evidence)**
> "[text]" ([date])
**üïµÔ∏è ÏõêÏù∏ Ï∂îÏ†ï**
- (Ï∂îÎ°†)
**üí° Í∞úÏÑ† ÏÜîÎ£®ÏÖò**
- **üîß Tech**: (Ï†úÏïà)
- **üé® UX/UI**: (Ï†úÏïà)

## 3. üèÜ Í∏çÏ†ï ÏöîÏÜå Î∞è Í∞ïÌôî Ï†ÑÎûµ
### 3.1 [Aspect Ïù¥Î¶Ñ] (Í∏çÏ†ï ÏöîÏÜå)
**üí¨ Ïú†Ï†ÄÏùò Î™©ÏÜåÎ¶¨**
> "[text]" ([date])
**üöÄ Í∞ïÌôî Î∞è ÎßàÏºÄÌåÖ Ï†ÑÎûµ**
- (Ï†úÏïà)

## 4. üìù Ï¥ùÌèâ Î∞è Îã§Ïùå Î≤ÑÏ†Ñ Ï†úÏïà
- (Î∞©Ìñ•ÏÑ± Ï†úÏãú)
"""
    try:
        model = genai.GenerativeModel('gemini-2.0-flash')
        response = model.generate_content(
            prompt, 
            generation_config=genai.types.GenerationConfig(temperature=0.2)
        )
        return response.text
    except Exception as e:
        print(f"‚ùå AI ÏÉùÏÑ± Ïã§Ìå®: {e}")
        return None

def process_single_version(v_idx, app_name, version, genre, 
                          vector_store_instance, reranker_instance):
    """Îã®Ïùº Î≤ÑÏ†Ñ Ï≤òÎ¶¨ (ÏµúÏ†ÅÌôî: Ï§ëÎ≥µ Ï≤¥ÌÅ¨ Ï†úÍ±∞Îê®)"""
    try:
        # DBÏóêÏÑú Ïù¥ÎØ∏ ÌïÑÌÑ∞ÎßÅÌñàÏúºÎØÄÎ°ú Ï¶âÏãú ÌÜµÍ≥Ñ Ï°∞Ìöå
        base_stats, aspect_stats = get_version_statistics(app_name, version)
        
        # ÏïàÏ†ÑÏû•Ïπò: ÌòπÏãúÎùºÎèÑ ÌÜµÍ≥ÑÍ∞Ä ÎπÑÏñ¥ÏûàÏúºÎ©¥ Ïä§ÌÇµ
        if not base_stats or not base_stats['total']:
            print(f"  ‚ö†Ô∏è [{app_name} v{version}] ÌÜµÍ≥Ñ Îç∞Ïù¥ÌÑ∞ ÏóÜÏùå (Skipping)")
            return False
        
        print(f"  üîÑ [{app_name} v{version}] Ï≤òÎ¶¨ ÏãúÏûë (Î¶¨Î∑∞: {base_stats['total']}Í∞ú, ÌèâÏ†ê: {base_stats['avg_rating']:.2f})")

        total_reviews = base_stats['total']
        avg_rating = base_stats['avg_rating'] or 0.0
        global_used_texts = set()
        rag_data = {}
        
        sorted_aspects = sorted(
            aspect_stats, 
            key=lambda x: (x['neg_count'], x['total_count']), 
            reverse=True
        )[:6]

        # RAG Îç∞Ïù¥ÌÑ∞ ÏàòÏßë
        for idx, stat in enumerate(sorted_aspects, 1):
            aspect = stat['aspect']
            total_cnt = stat['total_count']
            neg_count = stat['neg_count']
            neg_ratio = round((neg_count / total_cnt) * 100, 1) if total_cnt > 0 else 0.0
            
            print(f"    ‚îî Aspect {idx}/6: {aspect} (Î∂ÄÏ†ï {neg_ratio}%)", end=" ")
            
            aspect_data = {
                "stats": {"total": total_cnt, "neg_ratio": f"{neg_ratio}%"},
                "negative_reviews": [],
                "positive_reviews": []
            }
            
            if neg_count > 0:
                neg_vocs = get_rag_contexts(
                    app_name, version, aspect, "Î∂ÄÏ†ï", global_used_texts,
                    vector_store_instance, reranker_instance, top_k=3
                )
                aspect_data["negative_reviews"] = [{"text": v['text'], "date": v['date']} for v in neg_vocs]
                print(f"Î∂ÄÏ†ï:{len(neg_vocs)}Í∞ú", end=" ")
                
            if (total_cnt - neg_count) > 0:
                pos_vocs = get_rag_contexts(
                    app_name, version, aspect, "Í∏çÏ†ï", global_used_texts,
                    vector_store_instance, reranker_instance, top_k=3
                )
                aspect_data["positive_reviews"] = [{"text": v['text'], "date": v['date']} for v in pos_vocs]
                print(f"Í∏çÏ†ï:{len(pos_vocs)}Í∞ú")
                
            if aspect_data["negative_reviews"] or aspect_data["positive_reviews"]:
                rag_data[aspect] = aspect_data
            else:
                print("Îç∞Ïù¥ÌÑ∞ ÏóÜÏùå")

        if not rag_data:
            print(f"  ‚ùå [{app_name} v{version}] RAG Îç∞Ïù¥ÌÑ∞ Î∂ÄÏ°±ÏúºÎ°ú Ïä§ÌÇµ")
            return False

        print(f"  ü§ñ AI Î≥¥Í≥†ÏÑú ÏÉùÏÑ± Ï§ë...", end=" ", flush=True)
        report_md = generate_ai_report(app_name, version, rag_data, total_reviews, avg_rating)
        
        if not report_md:
            print("Ïã§Ìå®!")
            return False
        
        print(f"ÏÑ±Í≥µ! ({len(report_md)}Ïûê)")

        print(f"  üíæ DB Ï†ÄÏû• Ï§ë...", end=" ", flush=True)
        success = save_report_to_db(v_idx, report_md)
        
        if success:
            print("ÏÑ±Í≥µ! ‚úÖ ÏôÑÎ£å")
        else:
            print("Ïã§Ìå®! ‚ùå Ï†ÄÏû• ÏóêÎü¨")
        
        del rag_data
        gc.collect()
        return success
        
    except Exception as e:
        print(f"\n  ‚ùå [{app_name} v{version}] Ï≤òÎ¶¨ Ï§ë ÏòàÏô∏: {e}")
        gc.collect()
        return False

def aggressive_gc():
    gc.collect()
    gc.collect()
    try:
        ctypes.CDLL("libc.so.6").malloc_trim(0)
    except:
        pass
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    time.sleep(1)

def main():
    global reranker, vector_store, embeddings
    
    aggressive_gc()
    if not initialize_runtime():
        return
    
    print(f"üîÑ Î∂ÑÏÑù ÏãúÏûë | Ïû•Î•¥: {TARGET_GENRE} | ÏµúÏÜå Î¶¨Î∑∞: {MIN_REVIEW_LINES}Ï§Ñ Ïù¥ÏÉÅ")
    print(f"‚öôÔ∏è CPU Ïä§Î†àÎìú: {torch.get_num_threads()}")
    
    # üöÄ DBÏóêÏÑú ÌïÑÌÑ∞ÎßÅÎêú Î™©Î°ù Í∞ÄÏ†∏Ïò§Í∏∞
    targets = get_target_versions(TARGET_GENRE, MIN_REVIEW_LINES)
    print(f"üìö Ïã§Ï†ú Î∂ÑÏÑù ÎåÄÏÉÅ: {len(targets)}Í∞ú (Ïù¥ÎØ∏ ÏôÑÎ£åÎêú Í±¥ Î∞è Îç∞Ïù¥ÌÑ∞ Î∂ÄÏ°± Ï†úÏô∏Îê®)")
    
    if not targets:
        print("‚úÖ Ï≤òÎ¶¨Ìï† ÎåÄÏÉÅÏù¥ ÏóÜÏäµÎãàÎã§. Ï¢ÖÎ£åÌï©ÎãàÎã§.")
        return

    # Î™®Îç∏ Î°úÎìú
    try:
        logger.info(f"üîÑ Embeddings Î°úÎìú...")
        embeddings = HuggingFaceEmbeddings(
            model_name=EMBEDDING_MODEL,
            model_kwargs={'device': device},
            encode_kwargs={'normalize_embeddings': True, 'batch_size': 16}
        )
        aggressive_gc()

        print(f"üîÑ Vector Store Î°úÎìú...", flush=True)
        vector_store = Chroma(
            persist_directory=PERSIST_DIRECTORY,
            embedding_function=embeddings,
            collection_name="review_sentences"
        )
        aggressive_gc()
        
        print(f"üîÑ Reranker Î°úÎìú...", flush=True)
        try:
            reranker = CrossEncoder(
                RERANKER_MODEL_NAME, 
                device=device,
                max_length=256,
                num_labels=1,
                automodel_args={'trust_remote_code': True}
            )
        except TypeError:
            reranker = CrossEncoder(
                RERANKER_MODEL_NAME, 
                device=device,
                max_length=256
            )
        aggressive_gc()

    except Exception as e:
        print(f"‚ùå Î™®Îç∏ Î°úÎìú Ïã§Ìå®: {e}")
        return
    
    success_count = 0
    skip_count = 0
    error_count = 0
    batch_counter = 0
    
    pbar = tqdm(targets, desc="ÏßÑÌñâ")
    
    for target in pbar:
        v_idx = target['v_idx']
        app_name = target['a_name']
        version = target['v_version']
        
        try:
            # Ï§ëÎ≥µ Ï≤¥ÌÅ¨ Î°úÏßÅÏù¥ Ï†úÍ±∞ÎêòÏñ¥ Î∞îÎ°ú Ï≤òÎ¶¨
            is_success = process_single_version(
                v_idx, app_name, version, target['ag_name'],
                vector_store, reranker
            )
            
            if is_success:
                success_count += 1
                pbar.set_postfix({"‚úÖ": success_count, "‚ùå": error_count})
            else:
                skip_count += 1
            
            batch_counter += 1
            if batch_counter >= MAX_BATCH_SIZE:
                aggressive_gc()
                time.sleep(SLEEP_BETWEEN_BATCH)
                batch_counter = 0
                
        except KeyboardInterrupt:
            print("\nüõë Ï§ëÎã®Îê®")
            break
        except Exception as e:
            error_count += 1
            print(f"\n‚ùå {app_name} v{version}: {e}")
            aggressive_gc()
            time.sleep(SLEEP_AFTER_ERROR)
            continue

    print("\nüßπ Ï†ïÎ¶¨ Ï§ë...")
    try:
        del reranker, vector_store, embeddings
    except: pass
    
    aggressive_gc()
    logger.info(f"‚úÖ ÏµúÏ¢Ö ÏôÑÎ£å: {success_count}Í∞ú | Ïã§Ìå®/Ïä§ÌÇµ: {skip_count+error_count}Í∞ú")

if __name__ == "__main__":
    main()